<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="DLPO: Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback">
  <meta name="keywords" content="DLPO, TTS, Diffusion Models, Reinforcement Learning, Human Feedback">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>DLPO: Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback</title>

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/bulma/0.9.4/css/bulma.min.css">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">

  <style>
    .publication-title {
      font-family: 'Google Sans', sans-serif;
    }
    
    .author-block {
      display: inline-block;
      margin-right: 10px;
    }
    
    .publication-authors {
      margin-top: 20px;
    }
    
    .publication-links {
      margin-top: 30px;
    }
    
    .dlpo {
      color: #ff6b6b;
      font-weight: bold;
    }
    
    .hero {
      background: white;
      color: black;
    }
    
    .method-comparison {
      background-color: #f8f9fa;
      padding: 20px;
      border-radius: 8px;
      margin: 20px 0;
    }
    
    .results-table {
      margin: 20px 0;
    }
    
    .demo-section {
      background: linear-gradient(135deg, #ffeaa7 0%, #fab1a0 100%);
      padding: 40px 0;
    }
  </style>
</head>
<body>

<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-2 publication-title">Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback</h1>
          
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="#" style="color: black;">Jingyi Chen</a><sup>1,2</sup>,</span>
            <span class="author-block">
              <a href="#" style="color: black;">Ju Seung Byun</a><sup>2</sup>,</span>
            <span class="author-block">
              <a href="#" style="color: black;">Micha Elsner</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="#" style="color: black;">Pichao Wang</a><sup>3</sup>,</span>
            <span class="author-block">
              <a href="#" style="color: black;">Andrew Perrault</a><sup>2</sup></span>
          </div>

          <div class="is-size-6 publication-authors" style="margin-top: 15px;">
            <span class="author-block"><sup>1</sup>Department of Linguistics, The Ohio State University</span><br>
            <span class="author-block"><sup>2</sup>Department of Computer Science and Engineering, The Ohio State University</span><br>
            <span class="author-block"><sup>3</sup>Amazon</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- Paper Link -->
              <span class="link-block">
                <a href="#"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper</span>
                </a>
              </span>
              
              <!-- Code Link -->
              <span class="link-block">
                <a href="https://github.com/DeliJingyiC/DLPO_code"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code</span>
                </a>
              </span>
              
              <!-- Demo Link -->
              <span class="link-block">
                <a href="https://demopagea.github.io/DLPO-demo/"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-volume-up"></i>
                  </span>
                  <span>Demo</span>
                </a>
              </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="demo-section">
  <div class="container is-max-desktop">
    <div class="hero-body has-text-centered">
      <h2 class="title is-3">üéµ Audio Demos</h2>
      <p class="subtitle">
        <span class="dlpo">DLPO</span> significantly improves speech quality and naturalness in TTS diffusion models.
        Listen to the difference!
      </p>
      <div class="buttons is-centered">
        <a href="https://demopagea.github.io/DLPO-demo/" class="button is-primary is-large">
          <span class="icon">
            <i class="fas fa-play"></i>
          </span>
          <span>Listen to Audio Samples</span>
        </a>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <!-- Abstract -->
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Diffusion models produce high-fidelity speech but are inefficient for real-time use due to long denoising steps and challenges in modeling intonation and rhythm. To improve this, we propose <strong>Diffusion Loss-Guided Policy Optimization (DLPO)</strong>, an RLHF framework for TTS diffusion models.
          </p>
          <p>
            DLPO integrates the original training loss into the reward function, preserving generative capabilities while reducing inefficiencies. Using naturalness scores as feedback, DLPO aligns reward optimization with the diffusion model's structure, improving speech quality.
          </p>
          <p>
            We evaluate DLPO on WaveGrad 2, a non-autoregressive diffusion-based TTS model. Results show significant improvements in objective metrics (UTMOS 3.65, NISQA 4.02) and subjective evaluations, with <span class="dlpo">DLPO</span> audio preferred <strong>67% of the time</strong>. These findings demonstrate DLPO's potential for efficient, high-quality diffusion TTS in real-time, resource-limited settings.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Method Overview</h2>
        
        <div class="method-comparison">
          <h3 class="title is-4">üîß Diffusion Loss-Guided Policy Optimization (DLPO)</h3>
          <div class="content">
            <p>
              Unlike existing RLHF methods that rely solely on external rewards or KL regularization, DLPO introduces a unique approach by directly integrating the diffusion model's original training loss into the reward function. This innovation serves two critical purposes:
            </p>
            <div class="columns">
              <div class="column">
                <div class="box">
                  <h4 class="title is-5">üéØ Preserve Model Capabilities</h4>
                  <p>By aligning the reward function with the original diffusion training objective, DLPO ensures the model maintains its ability to generate high-quality speech while adapting to human feedback.</p>
                </div>
              </div>
              <div class="column">
                <div class="box">
                  <h4 class="title is-5">üõ°Ô∏è Prevent Overfitting</h4>
                  <p>The original diffusion loss acts as a stabilizing regularizer, balancing external reward optimization with preservation of the model's probabilistic structure.</p>
                </div>
              </div>
            </div>
          </div>
        </div>

        <svg viewBox="0 0 1200 800" xmlns="http://www.w3.org/2000/svg">
  <defs>
    <!-- Gradients -->
    <linearGradient id="modelGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#FF6B9D;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#C44569;stop-opacity:1" />
    </linearGradient>
    
    <linearGradient id="rewardGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#A8E6CF;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#7FCDCD;stop-opacity:1" />
    </linearGradient>
    
    <linearGradient id="lossGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#FFD93D;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#FF8C42;stop-opacity:1" />
    </linearGradient>
    
    <linearGradient id="dlpoGrad" x1="0%" y1="0%" x2="100%" y2="100%">
      <stop offset="0%" style="stop-color:#6C5CE7;stop-opacity:1" />
      <stop offset="100%" style="stop-color:#A29BFE;stop-opacity:1" />
    </linearGradient>
    
    <!-- Arrow marker -->
    <marker id="arrowhead" markerWidth="10" markerHeight="7" 
     refX="9" refY="3.5" orient="auto">
      <polygon points="0 0, 10 3.5, 0 7" fill="#333" />
    </marker>
    
    <!-- Drop shadow filter -->
    <filter id="dropshadow" x="-20%" y="-20%" width="140%" height="140%">
      <feDropShadow dx="3" dy="3" stdDeviation="3" flood-color="#00000030"/>
    </filter>
  </defs>

  <!-- Title -->
  <text x="600" y="30" text-anchor="middle" font-family="Arial, sans-serif" font-size="24" font-weight="bold" fill="#333">
    DLPO: Diffusion Loss-Guided Policy Optimization
  </text>
  
  <!-- Input Text -->
  <rect x="50" y="100" width="120" height="60" rx="15" fill="#FFE0E6" stroke="#FF8A9B" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="110" y="125" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" font-weight="bold" fill="#D63384">Input Text</text>
  <text x="110" y="140" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="#D63384">"Hello world"</text>
  
  <!-- WaveGrad 2 Model -->
  <rect x="250" y="80" width="150" height="100" rx="20" fill="url(#modelGrad)" stroke="#C44569" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="325" y="105" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="white">WaveGrad 2</text>
  <text x="325" y="120" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" fill="white">TTS Model</text>
  <text x="325" y="135" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Diffusion Process</text>
  <text x="325" y="150" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">x_T ‚Üí x_0</text>
  
  <!-- Generated Audio -->
  <rect x="480" y="100" width="120" height="60" rx="15" fill="#E8F5FF" stroke="#74B9FF" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="540" y="125" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" font-weight="bold" fill="#0984E3">Generated</text>
  <text x="540" y="140" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" font-weight="bold" fill="#0984E3">Audio üéµ</text>
  
  <!-- UTMOS Reward Model -->
  <rect x="480" y="220" width="120" height="80" rx="15" fill="url(#rewardGrad)" stroke="#00B894" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="540" y="245" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="white">UTMOS ‚≠ê</text>
  <text x="540" y="260" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" fill="white">Reward Model</text>
  <text x="540" y="275" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Naturalness</text>
  <text x="540" y="290" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Score: r(x‚ÇÄ, c)</text>
  
  <!-- Original Diffusion Loss -->
  <rect x="250" y="220" width="150" height="80" rx="15" fill="url(#lossGrad)" stroke="#E17055" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="325" y="245" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="white">Diffusion Loss üîß</text>
  <text x="325" y="260" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="10" fill="white">||ŒµÃÉ(x_t,t) - Œµ_Œ∏(x_t,c,t)||¬≤</text>
  <text x="325" y="275" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Preserves Model</text>
  <text x="325" y="290" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Capabilities</text>
  
  <!-- DLPO Combined Objective -->
  <rect x="700" y="350" width="200" height="120" rx="20" fill="url(#dlpoGrad)" stroke="#6C5CE7" stroke-width="4" filter="url(#dropshadow)"/>
  <text x="800" y="380" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="17" font-weight="bold" fill="white">DLPO Objective üöÄ</text>
  <text x="800" y="400" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="white">-Œ±¬∑r(x‚ÇÄ,c) - Œ≤¬∑||ŒµÃÉ-Œµ_Œ∏||¬≤</text>
  <text x="800" y="420" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">‚ú® Reward Optimization</text>
  <text x="800" y="435" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">üõ°Ô∏è Capability Preservation</text>
  <text x="800" y="450" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">üéØ Prevents Overfitting</text>
  
  <!-- Policy Update -->
  <rect x="700" y="520" width="200" height="80" rx="15" fill="#00CEC9" stroke="#00B894" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="800" y="545" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="white">Policy Update üìà</text>
  <text x="800" y="565" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="13" fill="white">Œ∏ ‚Üê Œ∏ + ‚àá_Œ∏ J_DLPO(Œ∏)</text>
  <text x="800" y="580" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="white">Improved Speech Quality!</text>
  
  <!-- Feedback Loop -->
  <path d="M 800 520 Q 1000 450 1000 300 Q 1000 150 400 100 Q 350 90 325 90" 
        stroke="#6f42c1" stroke-width="3" fill="none" stroke-dasharray="5,5" marker-end="url(#arrowhead)"/>
  <text x="950" y="200" text-anchor="middle" font-family="Arial, sans-serif" font-size="12" fill="#6f42c1" font-weight="bold">
    Iterative Refinement
  </text>
  
  <!-- Arrows -->
  <!-- Input to Model -->
  <line x1="170" y1="130" x2="240" y2="130" stroke="#FF6B9D" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- Model to Audio -->
  <line x1="400" y1="130" x2="470" y2="130" stroke="#74B9FF" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- Audio to UTMOS -->
  <line x1="540" y1="160" x2="540" y2="210" stroke="#00CEC9" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- Model to Diffusion Loss -->
  <line x1="325" y1="180" x2="325" y2="210" stroke="#FFD93D" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- UTMOS to DLPO -->
  <line x1="600" y1="260" x2="680" y2="380" stroke="#A8E6CF" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- Diffusion Loss to DLPO -->
  <line x1="400" y1="260" x2="680" y2="380" stroke="#FF8C42" stroke-width="4" marker-end="url(#arrowhead)"/>
  
  <!-- DLPO to Policy Update -->
  <line x1="800" y1="470" x2="800" y2="510" stroke="#6C5CE7" stroke-width="4" marker-end="url(#arrowhead)"/>url(#arrowhead)"/>
  
  <!-- UTMOS to DLPO -->
  <line x1="600" y1="260" x2="680" y2="380" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Diffusion Loss to DLPO -->
  <line x1="400" y1="260" x2="680" y2="380" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- DLPO to Policy Update -->
  <line x1="800" y1="470" x2="800" y2="510" stroke="#333" stroke-width="2" marker-end="url(#arrowhead)"/>
  
  <!-- Key Innovation Box -->
  <rect x="50" y="350" width="280" height="120" rx="15" fill="#FFF9E6" stroke="#FFB347" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="190" y="375" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="#E67E22">
    üîë Key Innovation
  </text>
  <text x="190" y="395" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#E67E22">
    Integrating original diffusion loss
  </text>
  <text x="190" y="410" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#E67E22">
    into reward function prevents
  </text>
  <text x="190" y="425" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#E67E22">
    capability degradation while
  </text>
  <text x="190" y="440" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#E67E22">
    optimizing for human preferences! üéâ
  </text>
  
  <!-- Benefits Box -->
  <rect x="50" y="500" width="280" height="120" rx="15" fill="#E8F8FF" stroke="#74B9FF" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="190" y="525" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="#0984E3">
    üìà DLPO Benefits
  </text>
  <text x="190" y="545" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#0984E3">
    üéØ UTMOS: 2.90 ‚Üí 3.65 (+26%)
  </text>
  <text x="190" y="560" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#0984E3">
    üéµ NISQA: 3.74 ‚Üí 4.02 (+7.5%)
  </text>
  <text x="190" y="575" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#0984E3">
    üìù WER: 1.5% ‚Üí 1.2% (-20%)
  </text>
  <text x="190" y="590" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="12" fill="#0984E3">
    üë• 67% human preference! üéä
  </text>
  
  <!-- Mathematical Formula -->
  <rect x="50" y="650" width="800" height="80" rx="15" fill="#FFEAA7" stroke="#FDCB6E" stroke-width="3" filter="url(#dropshadow)"/>
  <text x="450" y="675" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="15" font-weight="bold" fill="#E17055">
    üßÆ DLPO Objective Function:
  </text>
  <text x="450" y="700" text-anchor="middle" font-family="Courier New, monospace" font-size="17" fill="#E17055" font-weight="bold">
    ùîº[‚àíŒ±¬∑r(x‚ÇÄ,c) ‚àí Œ≤¬∑‚ÄñŒµÃÉ(x‚Çú,t) ‚àí ŒµŒ∏(x‚Çú,c,t)‚Äñ¬≤]
  </text>
  <text x="200" y="715" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="#D63031">
    Œ±: reward weight ‚öñÔ∏è
  </text>
  <text x="450" y="715" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="#D63031">
    Œ≤: diffusion loss weight üîß
  </text>
  <text x="700" y="715" text-anchor="middle" font-family="Comic Sans MS, Arial, sans-serif" font-size="11" fill="#D63031">
    r(x‚ÇÄ,c): UTMOS score ‚≠ê
  </text>
  
  <!-- Process Flow Labels -->
  <text x="200" y="75" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#666" font-style="italic">
    1. Text Input
  </text>
  <text x="430" y="75" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#666" font-style="italic">
    2. Audio Generation
  </text>
  <text x="540" y="340" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#666" font-style="italic">
    3. Quality Assessment
  </text>
  <text x="800" y="340" text-anchor="middle" font-family="Arial, sans-serif" font-size="10" fill="#666" font-style="italic">
    4. Combined Optimization
  </text>
</svg>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <h2 class="title is-3 has-text-centered">Experimental Results</h2>
    
    <div class="results-table">
      <h3 class="title is-4">üìà Comparison of RL Fine-tuning Methods</h3>
      <div class="table-container">
        <table class="table is-striped is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Method</th>
              <th>UTMOS ‚Üë</th>
              <th>NISQA ‚Üë</th>
              <th>WER ‚Üì</th>
            </tr>
          </thead>
          <tbody>
            <tr style="background-color: #e8f5e8;">
              <td><strong>Ground Truth</strong></td>
              <td>4.20</td>
              <td>4.37</td>
              <td>0.99%</td>
            </tr>
            <tr>
              <td>WaveGrad 2R (Baseline)</td>
              <td>2.90</td>
              <td>3.74</td>
              <td>1.5%</td>
            </tr>
            <tr>
              <td>RWR</td>
              <td>2.18</td>
              <td>3.00</td>
              <td>8.9%</td>
            </tr>
            <tr>
              <td>DDPO</td>
              <td>2.69</td>
              <td>2.96</td>
              <td>2.1%</td>
            </tr>
            <tr>
              <td>DPOK</td>
              <td>3.18</td>
              <td>3.76</td>
              <td>1.1%</td>
            </tr>
            <tr>
              <td>KLinR</td>
              <td>3.02</td>
              <td>3.73</td>
              <td>1.3%</td>
            </tr>
            <tr style="background-color: #fff3cd; font-weight: bold;">
              <td><strong>DLPO (Ours)</strong></td>
              <td><strong>3.65</strong></td>
              <td><strong>4.02</strong></td>
              <td><strong>1.2%</strong></td>
            </tr>
          </tbody>
        </table>
      </div>
    </div>

    <div class="columns">
      <div class="column">
        <div class="box">
          <h3 class="title is-4">üéß Human Evaluation Results</h3>
          <div class="content">
            <ul>
              <li><strong>67%</strong> of listeners preferred DLPO-generated audio</li>
              <li><strong>14%</strong> preferred baseline WaveGrad 2R</li>
              <li><strong>19%</strong> rated as about the same</li>
              <li>Statistical significance: <em>p</em> &lt; 10<sup>-16</sup> (binomial test)</li>
            </ul>
          </div>
        </div>
      </div>
      
      <div class="column">
        <div class="box">
          <h3 class="title is-4">‚ö° Key Improvements</h3>
          <div class="content">
            <ul>
              <li><strong>+26%</strong> improvement in UTMOS score</li>
              <li><strong>+7.5%</strong> improvement in NISQA score</li>
              <li><strong>20%</strong> reduction in word error rate</li>
              <li>Maintains computational efficiency</li>
            </ul>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-full-width">
        <h2 class="title is-3">Related Work & Discussion</h2>
        
        <div class="content has-text-justified">
          <p>
            This work addresses the unique challenges of fine-tuning TTS diffusion models using reinforcement learning techniques. While existing methods like RWR and DDPO struggle with the temporal and acoustic demands of TTS, <span class="dlpo">DLPO</span> provides a tailored solution.
          </p>
          
          <p>
            By integrating the diffusion model's original training loss into the reward function, DLPO stabilizes training, prevents overfitting, and enables task-specific adaptations. This approach demonstrates the importance of leveraging task-specific regularization to address the complexities of sequential data generation.
          </p>
          
          <p>
            Our findings establish DLPO as a robust framework for advancing diffusion-based TTS synthesis and set a foundation for broader applications in resource-constrained and real-time scenarios.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code>@article{chen2024dlpo,
  author    = {Chen, Jingyi and Byun, Ju Seung and Elsner, Micha and Wang, Pichao and Perrault, Andrew},
  title     = {Fine-Tuning Text-to-Speech Diffusion Models Using Reinforcement Learning with Human Feedback},
  journal   = {arXiv preprint},
  year      = {2024},
}</code></pre>
  </div>
</section>

<footer class="footer">
  <div class="container">
    <div class="content has-text-centered">
      <p>
        This website is based on the <a href="https://github.com/nerfies/nerfies.github.io">Nerfies template</a>.
        We thank the authors for making their template available.
      </p>
      <p>
        For questions about this work, please contact the authors at The Ohio State University.
      </p>
    </div>
  </div>
</footer>

</body>
</html>
